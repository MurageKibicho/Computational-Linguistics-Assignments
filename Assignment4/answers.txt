Question 1
For Aesop.trans, 2 generates longer sentences while 3 generates shortes sentences, mostly with 3 phenomes.
For Kf1002.trans, both generate much longer sentences/
Lexicon.trans generates sentences longer than Aesop and Kf1000 for both 2 and 3

Question 2
Yes, the models distinguish good words and bad words. This is seen with perplexities of 0 being assigned to a bigram of Aesop with badwords while a perplexity of 0.012 is assigned to a bigram of Aesop with goodwords. The distinction bettween the models is seen with trigram assigning higher perplexities than bigrams in some cases, for instance, it assigns a perplexity of 0.05 to Aesop with goodwords, compared to bigram's 0.012.
 
 On average, I don't see much of a distinction. For both bigrams and trigrams, training files run with badwords return low perplexities, close to zero while this with goodwords return higher perplexities.
 
 When we use a lexicon with unique words, like lexicon.trans bigram assigns higher probabilities than aesop and the probabilities seem close to those assigned by trigram but with something like aesop, bigram assigns low probabilities and trigram assigns higher probabilities

Question 3
Adding1 smoothing raises my probabilites and perplexities. There isn't much of a change except I notice that my values for the bigrams and trigrams are less different


